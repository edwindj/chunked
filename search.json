[{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Edwin de Jonge. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"de Jonge E (2026). chunked: Chunkwise Text-File Processing 'dplyr'. R package version 0.6.2, https://github.com/edwindj/chunked.","code":"@Manual{,   title = {chunked: Chunkwise Text-File Processing for 'dplyr'},   author = {Edwin {de Jonge}},   year = {2026},   note = {R package version 0.6.2},   url = {https://github.com/edwindj/chunked}, }"},{"path":"/index.html","id":"chunked","dir":"","previous_headings":"","what":"Chunkwise Text-File Processing for dplyr","title":"Chunkwise Text-File Processing for dplyr","text":"R great tool, processing data large text files cumbersome. chunked helps process large text files dplyr loading part data memory. builds excellent R package LaF. Processing commands written dplyr syntax, chunked (using LaF) take care chunk chunk processed, taking far less memory otherwise. chunked useful select-ing columns, mutate-ing columns filter-ing rows. less helpful group-ing summarize-ation large text files. can used data pre-processing.","code":""},{"path":"/index.html","id":"install","dir":"","previous_headings":"","what":"Install","title":"Chunkwise Text-File Processing for dplyr","text":"‘chunked’ can installed beta version : development version : Enjoy! Feedback welcome…","code":"install.packages('chunked') install.packages('chunked', repos=c('https://cran.rstudio.com', 'https://edwindj.github.io/drat')) devtools::install_github('edwindj/chunked')"},{"path":[]},{"path":"/index.html","id":"text-file---process---text-file","dir":"","previous_headings":"","what":"Text file -> process -> text file","title":"Chunkwise Text-File Processing for dplyr","text":"common case processing large text file, select add columns, filter write result back text file chunked write process statement chunks 5000 records. different example read.csv reads data memory processing .","code":"read_chunkwise(\"./large_file_in.csv\", chunk_size=5000) %>%    select(col1, col2, col5) %>%   filter(col1 > 10) %>%    mutate(col6 = col1 + col2) %>%    write_chunkwise(\"./large_file_out.csv\")"},{"path":"/index.html","id":"text-file---process---database","dir":"","previous_headings":"","what":"Text file -> process -> database","title":"Chunkwise Text-File Processing for dplyr","text":"Another option use chunked preprocessing step adding database","code":"con <- DBI::dbConnect(RSQLite::SQLite(), 'test.db', create=TRUE) db <- dbplyr::src_dbi(con)  tbl <-    read_chunkwise(\"./large_file_in.csv\", chunk_size=5000) %>%    select(col1, col2, col5) %>%   filter(col1 > 10) %>%    mutate(col6 = col1 + col2) %>%    write_chunkwise(dbplyr::src_dbi(db), 'my_large_table')    # tbl now points to the table in sqlite."},{"path":"/index.html","id":"db---process---text-file","dir":"","previous_headings":"","what":"Db -> process -> Text file","title":"Chunkwise Text-File Processing for dplyr","text":"Chunked can used export chunkwise text file. Note however case processing takes place database chunkwise restrictions apply writing.","code":""},{"path":"/index.html","id":"lazy-processing","dir":"","previous_headings":"","what":"Lazy processing","title":"Chunkwise Text-File Processing for dplyr","text":"chunked start processing collect write_chunkwise called. Syntax completion variables chunkwise file RStudio works like charm…","code":"data_chunks <-    read_chunkwise(\"./large_file_in.csv\", chunk_size=5000) %>%    select(col1, col3)    # won't start processing until collect(data_chunks) # or write_chunkwise(data_chunks, \"test.csv\") # or write_chunkwise(data_chunks, db, \"test\")"},{"path":"/index.html","id":"dplyr-verbs","dir":"","previous_headings":"","what":"Dplyr verbs","title":"Chunkwise Text-File Processing for dplyr","text":"chunked implements following dplyr verbs: filter select rename mutate mutate_each transmute tbl_vars inner_join left_join semi_join anti_join Since data processed chunks, dplyr verbs implemented: arrange right_join full_join summarize group_by implemented generate warning: operate chunk whole data set. However makes easy process large file, repeatedly aggregating resulting data. summarize group_by","code":"tmp <- tempfile() write.csv(iris, tmp, row.names=FALSE, quote=FALSE) iris_cw <- read_chunkwise(tmp, chunk_size = 30) # read in chunks of 30 rows for this example  iris_cw %>%    group_by(Species) %>%            # group in each chunk   summarise( m = mean(Sepal.Width) # and summarize in each chunk            , w = n()            ) %>%    as.data.frame %>%                  # since each Species has 50 records, results will be in multiple chunks   group_by(Species) %>%              # group the results from the chunk   summarise(m = weighted.mean(m, w)) # and summarize it again"},{"path":"/reference/chunked-package.html","id":null,"dir":"Reference","previous_headings":"","what":"Chunked — chunked-package","title":"Chunked — chunked-package","text":"R great tool, processing large text files data cumbersome. chunked helps process large text files dplyr loading part data memory. builds execellent R package LaF Processing commands writing dplyr syntax, chunked  (using LaF)  take care chunk chunk processed, taking far less memory  otherwise. chunked useful selecting columns, mutating columns  filtering rows. can used data pre-processing.","code":""},{"path":"/reference/chunked-package.html","id":"implemented-dplyr-verbs","dir":"Reference","previous_headings":"","what":"Implemented dplyr verbs","title":"Chunked — chunked-package","text":"filter select rename mutate transmute left_join inner_join anti_join semi_join tbl_vars collect filter, select, , left_join, inner_join","code":""},{"path":"/reference/chunked-package.html","id":"not-implemented","dir":"Reference","previous_headings":"","what":"Not implemented","title":"Chunked — chunked-package","text":"following operators implemented data chunked processed  chunkwise, available. full_join right_join group_by arrange tail","code":""},{"path":[]},{"path":"/reference/chunked-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Chunked — chunked-package","text":"Maintainer: Edwin de Jonge edwindjonge@gmail.com (ORCID)","code":""},{"path":"/reference/insert_chunkwise_into.html","id":null,"dir":"Reference","previous_headings":"","what":"insert data in chunks into a database — insert_chunkwise_into","title":"insert data in chunks into a database — insert_chunkwise_into","text":"insert_chunkwise_into can used insert chunks data database. Typically chunked can used preprocessing data adding database.","code":""},{"path":"/reference/insert_chunkwise_into.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"insert data in chunks into a database — insert_chunkwise_into","text":"","code":"insert_chunkwise_into(x, dest, table, temporary = FALSE, analyze = FALSE)"},{"path":"/reference/insert_chunkwise_into.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"insert data in chunks into a database — insert_chunkwise_into","text":"x tbl_chunk object dest database destination, e.g. src_dbi() table name table temporary table removed database connection closed? analyze table analyzed import?","code":""},{"path":"/reference/insert_chunkwise_into.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"insert data in chunks into a database — insert_chunkwise_into","text":"tbl object pointing table database dest.","code":""},{"path":"/reference/read_chunks.html","id":null,"dir":"Reference","previous_headings":"","what":"Read chunkwise data from text files — read_csv_chunkwise","title":"Read chunkwise data from text files — read_csv_chunkwise","text":"read_csv_chunk open connection text file. Subsequent dplyr verbs commands recorded collect, write_csv_chunkwise called. case recorded commands executed chunk chunk. ","code":""},{"path":"/reference/read_chunks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read chunkwise data from text files — read_csv_chunkwise","text":"","code":"read_csv_chunkwise(   file,   chunk_size = 10000L,   header = TRUE,   sep = \",\",   dec = \".\",   stringsAsFactors = FALSE,   ... )  read_csv2_chunkwise(   file,   chunk_size = 10000L,   header = TRUE,   sep = \";\",   dec = \",\",   ... )  read_table_chunkwise(   file,   chunk_size = 10000L,   header = TRUE,   sep = \" \",   dec = \".\",   ... )  read_laf_chunkwise(laf, chunk_size = 10000L)"},{"path":"/reference/read_chunks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read chunkwise data from text files — read_csv_chunkwise","text":"file path texst file chunk_size size chunks te read header csv file header column names? sep field separator used dec decimal separator used stringsAsFactors logical string read factors? ... used read_laf_chunkwise reads chunkwise LaF object created laf_open. offers control data specification. laf laf object created using LaF","code":""},{"path":"/reference/read_chunks.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Read chunkwise data from text files — read_csv_chunkwise","text":"read_csv_chunkwise can best combined write_csv_chunkwise insert_chunkwise_into (see example)","code":""},{"path":"/reference/read_chunks.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read chunkwise data from text files — read_csv_chunkwise","text":"","code":"# create csv file for demo purpose in_file <- file.path(tempdir(), \"in.csv\") write.csv(women, in_file, row.names = FALSE, quote = FALSE)  # women_chunked <-   read_chunkwise(in_file) %>%  #open chunkwise connection   mutate(ratio = weight/height) %>%   filter(ratio > 2) %>%   select(height, ratio) %>%   inner_join(data.frame(height=63:66)) # you can join with data.frames!  # no processing done until out_file <- file.path(tempdir(), \"processed.csv\") women_chunked %>%   write_chunkwise(file=out_file) #> Joining with `by = join_by(height)`  head(women_chunked) # works (without processing all data...) #> Joining with `by = join_by(height)` #>   height    ratio #> 1     63 2.047619 #> 2     64 2.062500 #> 3     65 2.076923 #> 4     66 2.106061  iris_file <- file.path(tempdir(), \"iris.csv\") write.csv(iris, iris_file, row.names = FALSE, quote= FALSE)  iris_chunked <-   read_chunkwise(iris_file, chunk_size = 49) %>% # 49 for demo purpose   group_by(Species) %>%   summarise(sepal_length = mean(Sepal.Length), n=n()) # note that mean is per chunk"},{"path":"/reference/read_chunkwise.html","id":null,"dir":"Reference","previous_headings":"","what":"Read chunkwise from a data source — read_chunkwise","title":"Read chunkwise from a data source — read_chunkwise","text":"Read chunkwise data source","code":""},{"path":"/reference/read_chunkwise.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read chunkwise from a data source — read_chunkwise","text":"","code":"read_chunkwise(src, chunk_size = 10000L, ...)  # S3 method for class 'character' read_chunkwise(   src,   chunk_size = 10000L,   format = c(\"csv\", \"csv2\", \"table\"),   stringsAsFactors = FALSE,   ... )  # S3 method for class 'laf' read_chunkwise(src, chunk_size = 10000L, ...)  # S3 method for class 'tbl_sql' read_chunkwise(src, chunk_size = 10000L, ...)"},{"path":"/reference/read_chunkwise.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read chunkwise from a data source — read_chunkwise","text":"src source read chunk_size size chunks ... parameters used specific classes format used specifying type text file stringsAsFactors logical string read factors?","code":""},{"path":"/reference/read_chunkwise.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read chunkwise from a data source — read_chunkwise","text":"object type tbl_chunk","code":""},{"path":"/reference/write_chunkwise.html","id":null,"dir":"Reference","previous_headings":"","what":"Genereric function to write chunk by chunk — write_chunkwise","title":"Genereric function to write chunk by chunk — write_chunkwise","text":"Genereric function write chunk chunk","code":""},{"path":"/reference/write_chunkwise.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Genereric function to write chunk by chunk — write_chunkwise","text":"","code":"write_chunkwise(x, dest, ...)  # S3 method for class 'chunkwise' write_chunkwise(   x,   dest,   table,   file = dest,   format = c(\"csv\", \"csv2\", \"table\"),   ... )"},{"path":"/reference/write_chunkwise.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Genereric function to write chunk by chunk — write_chunkwise","text":"x chunked input, e.g. created read_chunkwise can tbl_sql object. dest data written. May character src_sql. ... parameters passed specific implementations. table table write . used dest data base(src_sql) file File write format Specifies text format written disk. used x character.","code":""},{"path":"/reference/write_csv.html","id":null,"dir":"Reference","previous_headings":"","what":"Write chunks to a csv file — write_csv_chunkwise","title":"Write chunks to a csv file — write_csv_chunkwise","text":"Writes data csv file chunk chunk. function must just conjunction read_csv_chunkwise. Chunks data read, processed written function called. writing database use insert_chunkwise_into.","code":""},{"path":"/reference/write_csv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write chunks to a csv file — write_csv_chunkwise","text":"","code":"write_csv_chunkwise(   x,   file = \"\",   sep = \",\",   dec = \".\",   col.names = TRUE,   row.names = FALSE,   ... )  write_csv2_chunkwise(   x,   file = \"\",   sep = \";\",   dec = \",\",   col.names = TRUE,   row.names = FALSE,   ... )  write_table_chunkwise(   x,   file = \"\",   sep = \"\\t\",   dec = \".\",   col.names = TRUE,   row.names = TRUE,   ... )"},{"path":"/reference/write_csv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write chunks to a csv file — write_csv_chunkwise","text":"x chunkwise object pointing text file file file character connection csv file written sep field separator dec decimal separator col.names column names written? row.names row names written? ... passed read.table","code":""},{"path":"/reference/write_csv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write chunks to a csv file — write_csv_chunkwise","text":"chunkwise object (chunkwise), writing file refers newly created file, otherwise x.","code":""},{"path":"/reference/write_csv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write chunks to a csv file — write_csv_chunkwise","text":"","code":"# create csv file for demo purpose in_file <- file.path(tempdir(), \"in.csv\") write.csv(women, in_file, row.names = FALSE, quote = FALSE)  # women_chunked <-   read_chunkwise(in_file) %>%  #open chunkwise connection   mutate(ratio = weight/height) %>%   filter(ratio > 2) %>%   select(height, ratio) %>%   inner_join(data.frame(height=63:66)) # you can join with data.frames!  # no processing done until out_file <- file.path(tempdir(), \"processed.csv\") women_chunked %>%   write_chunkwise(file=out_file) #> Joining with `by = join_by(height)`  head(women_chunked) # works (without processing all data...) #> Joining with `by = join_by(height)` #>   height    ratio #> 1     63 2.047619 #> 2     64 2.062500 #> 3     65 2.076923 #> 4     66 2.106061  iris_file <- file.path(tempdir(), \"iris.csv\") write.csv(iris, iris_file, row.names = FALSE, quote= FALSE)  iris_chunked <-   read_chunkwise(iris_file, chunk_size = 49) %>% # 49 for demo purpose   group_by(Species) %>%   summarise(sepal_length = mean(Sepal.Length), n=n()) # note that mean is per chunk"},{"path":"/news/index.html","id":"version-062","dir":"Changelog","previous_headings":"","what":"Version 0.6.2","title":"Version 0.6.2","text":"changed argument add .add group_by changed argument keep .keep group_split","code":""},{"path":"/news/index.html","id":"version-06","dir":"Changelog","previous_headings":"","what":"Version 0.6","title":"Version 0.6","text":"removed dependency trunc_mat (change dplyr)","code":""},{"path":"/news/index.html","id":"version-051","dir":"Changelog","previous_headings":"","what":"Version 0.5.1","title":"Version 0.5.1","text":"CRAN release: 2020-11-03 Use DBI functions db access instead dbplyr versions. Thanks @hadley Changed default settings stringsAsFactors FALSE, compliant R version 4.0","code":""},{"path":"/news/index.html","id":"version-050","dir":"Changelog","previous_headings":"","what":"Version 0.5.0","title":"Version 0.5.0","text":"CRAN release: 2020-03-24 Fix release dplyr 1.0.0","code":""},{"path":"/news/index.html","id":"version-041","dir":"Changelog","previous_headings":"","what":"Version 0.4.1","title":"Version 0.4.1","text":"CRAN release: 2020-03-08 Fix change default stringsAsFactors R 4.0 Implementation now uses rlang instead lazyeval Added stringsAsFactors argument read_chunkwise functions.","code":""},{"path":"/news/index.html","id":"version-031","dir":"Changelog","previous_headings":"","what":"Version 0.3.1","title":"Version 0.3.1","text":"Fix dplyr upgrade 0.5 0.6","code":""},{"path":"/news/index.html","id":"version-021","dir":"Changelog","previous_headings":"","what":"Version 0.2.1","title":"Version 0.2.1","text":"CRAN release: 2016-04-07 Updated tests testthat changes","code":""},{"path":"/news/index.html","id":"version-020","dir":"Changelog","previous_headings":"","what":"Version 0.2.0","title":"Version 0.2.0","text":"CRAN release: 2016-03-23 implemented summarize group_by per chunk. fixed bug head (n working)","code":""}]
